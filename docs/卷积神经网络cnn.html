<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>卷积神经网络CNN - Laumy的技术栈</title>
    <link rel="stylesheet" href="assets/style.css">
    <!-- MathJax支持LaTeX数学公式 -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          ignoreHtmlClass: 'tex2jax_ignore',
          processHtmlClass: 'tex2jax_process'
        }
      };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <header class="site-header">
      <div class="container">
        <a class="logo" href="./">Laumy的技术栈</a>
        <div class="search">
          <input id="search-input" type="text" placeholder="输入关键词回车搜索">
        </div>
        <div class="theme-toggle" title="切换主题" id="theme-toggle">☾</div>
        <nav class="top-nav">
          <div class="nav-item"><a href="./">首页</a></div>
        </nav>
      </div>
    </header>

    <main class="container layout">
      <aside class="left-nav">
        
        <div class="card">
          <div class="card-title">文章目录</div>
          <nav id="toc"><ul><li><a href="#_1">图像卷积</a><ul></ul></li><li><a href="#pooling">池化pooling</a><ul></ul></li><li><a href="#lenet">LeNet网络</a><ul></ul></li></ul></nav>
        </div>
        
      </aside>

      <section class="content">
        
<article class="card post">
  <h1>卷积神经网络CNN</h1>
  <div class="meta">2025-05-06 · ai</div>
  <div class="post-content"><h2 id="_1">图像卷积</h2>
<p>图像卷积是有一个卷积核，这个卷积核对输入做相关运算。卷积核从输入的张量左上角开始、从左到右、从上到下进行滑动，每到一个位置时，在该窗口的部分张量与卷积核做点积得到一个输出。</p>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/05/wp_editor_md_5a5c8c18a3810713bcf153abf8162bd2.jpg"><img alt="" src="assets/doc/04-ai/深度学习/卷积神经网络cnn/images/wp_editor_md_5a5c8c18a3810713bcf153abf8162bd2.jpg"/></a></p>
<p>为什么要使用卷积了，主要是要解决以下问题</p>
<ul>
<li>参数爆炸问：传统全连接网络处理图像时参数规模过大（如1000×1000像素图像需30亿参数），而CNN通过局部连接和权值共享大幅减少参数数量23。</li>
<li>平移不变性缺：卷积核的滑动扫描机制使CNN能识别不同位置的相同特征。</li>
<li>局部相关性建：通过卷积操作捕捉相邻像素间的空间关联性。</li>
</ul>
<p>如果是多个输入通道，比如图片RGB 3个通道，那么核函数对应有3个核函数，下面是2个通道的示例。</p>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/05/wp_editor_md_3191957d249a065d69d24f3297886f86.jpg"><img alt="" src="assets/doc/04-ai/深度学习/卷积神经网络cnn/images/wp_editor_md_3191957d249a065d69d24f3297886f86.jpg"/></a></p>
<p>卷积核放在神经网络里，就代表对应的权重（weight)，卷积网络可以起到提取图像特征的作用。</p>
<h2 id="pooling">池化pooling</h2>
<p>在处理图像时，每个像素的变化会导致参数变化也比较大，随着神经网络层数的上升，每个神经元对其敏感的感受就越大，这样对训练不一定是件好事情。为了减低卷积层对位置的敏感性，可以通过再加一层池化层来解决，池化一般有最大值和平均值两种方法。</p>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/05/wp_editor_md_fa2998fd7637ec2840c6dccee4178876.jpg"><img alt="" src="assets/doc/04-ai/深度学习/卷积神经网络cnn/images/wp_editor_md_fa2998fd7637ec2840c6dccee4178876.jpg"/></a></p>
<ul>
<li>maximum pooling: 取池化层窗口的最大值</li>
<li>average pooling: 取池化层窗口的平均值。</li>
</ul>
<p>池化层与卷积的运算类似，只不过运算不一样。池化窗口从输入张量的左上角开始、从左往右、从上往下的在输入张量内滑动。在池化窗口达到的位置，计算该窗口的子张量最大值或者平均值。</p>
<h2 id="lenet">LeNet网络</h2>
<p>LeNet是最早的卷积神经网络，在1989年广泛运用在自动取款机中。</p>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/05/wp_editor_md_c7853efc44f739d5c79fde5dfaac1516.jpg"><img alt="" src="assets/doc/04-ai/深度学习/卷积神经网络cnn/images/wp_editor_md_c7853efc44f739d5c79fde5dfaac1516.jpg"/></a></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="c1">#第一个参数1表示输入通道，6表示输出通道。这里的通道指的</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</code></pre></div>
<p>本文来自： <a href="https://zh.d2l.ai/" title="&lt;动手学深度学习 V2&gt;">&lt;动手学深度学习 V2&gt;</a> 的学习笔记</p></div>
  <div class="post-nav">
    <a class="prev" href="现代卷积神经网络.html">← 现代卷积神经网络</a>
    <a class="next" href="非暴力沟通.html">非暴力沟通 →</a>
  </div>
</article>

      </section>

      <aside class="right-panel">
        
      </aside>
    </main>

    <footer class="site-footer">
      <div class="container">Copyright ©2022-2025 laumy 版权所有</div>
    </footer>

    <script src="assets/site.js"></script>
  </body>
  </html>

