<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>激活函数 - Laumy的技术栈</title>
    <link rel="stylesheet" href="/assets/style.css">
    <!-- MathJax支持LaTeX数学公式 -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          ignoreHtmlClass: 'tex2jax_ignore',
          processHtmlClass: 'tex2jax_process'
        }
      };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <header class="site-header">
      <div class="container">
        <a class="logo" href="/">Laumy的技术栈</a>
        <div class="search">
          <input id="search-input" type="text" placeholder="输入关键词回车搜索">
        </div>
        <div class="theme-toggle" title="切换主题" id="theme-toggle">☾</div>
        <nav class="top-nav">
          <div class="nav-item"><a href="/">首页</a></div>
        </nav>
      </div>
    </header>

    <main class="container layout">
      <aside class="left-nav">
        
        <div class="card">
          <div class="card-title">文章目录</div>
          <nav id="toc"><ul><li><a href="#_1">概念</a><ul></ul></li><li><a href="#_2">常见的激活函数</a><ul><li><a href="#relu">ReLU 激活函数</a></li><li><a href="#sigmoid">Sigmoid 激活函数</a></li><li><a href="#tanh">Tanh 激活函数</a></li><li><a href="#softmax">Softmax 激活函数</a></li></ul></li></ul></nav>
        </div>
        
      </aside>

      <section class="content">
        
<article class="card post">
  <h1>激活函数</h1>
  <div class="meta">2025-04-30 · ai</div>
  <div class="post-content"><h2 id="_1">概念</h2>
<p>前面我们主要使用的是线性模型，但是线性模型有很多局限性，因为我们要建模的问题并不能单纯使用线性模型就能够拟合的，如下示例。</p>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/04/wp_editor_md_2da7e842d509202e10eaf5e1a70a177c.jpg"><img alt="" src="/assets/doc/04-ai/深度学习/激活函数/images/wp_editor_md_2da7e842d509202e10eaf5e1a70a177c.jpg"/></a></p>
<p>我们要拟合红色部分的函数，使用线性模型即使在怎么调整W和b都没法进行拟合出来，要拟合这样的函数，我们需要非线性的函数。</p>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/04/wp_editor_md_dc886f7f7fafc2e5ac7cc9de7b9b17ca.jpg"><img alt="" src="/assets/doc/04-ai/深度学习/激活函数/images/wp_editor_md_dc886f7f7fafc2e5ac7cc9de7b9b17ca.jpg"/></a></p>
<p>如上图，要拟合这样的模型，我们可以使用①②③函数相加再加上一个b偏置。那这里的①②③函数怎么来了，可以看出是wx+b再经过一个sigmoid转换得来，那这里的sigmoid我们就称为激活函数。</p>
<p>激活函数的主要作用是引入非线性，使得神经网络能够处理更复杂的问题并避免退化成线性模型。没有激活函数，神经网络就无法发挥其强大的学习和表达能力。选择合适的激活函数对模型的训练和性能表现至关重要。</p>
<h2 id="_2">常见的激活函数</h2>
<h3 id="relu">ReLU 激活函数</h3>
<p>公式：$ \text{ReLU}(x) = \max(0, x) $</p>
<div class="codehilite"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">8.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="s1">'x'</span><span class="p">,</span> <span class="s1">'relu(x)'</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
</code></pre></div>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/04/wp_editor_md_d782a614f3890ad1079aaf53256966e1.jpg"><img alt="" src="/assets/doc/04-ai/深度学习/激活函数/images/wp_editor_md_d782a614f3890ad1079aaf53256966e1.jpg"/></a></p>
<p>ReLU激活函数用得比较多，因为其计算相对简单，不需要复杂的指数计算，因为指数计算都很贵。</p>
<p>ReLU函数进行求导，可以发现当输入为负时，导数为0，当输入为正是，导数为1。可以使用y.backward来计算导数，可以理解导数就是梯度。x取不同位置进行求导得到的值，就是相应位置的梯度。</p>
<div class="codehilite"><pre><span></span><code><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="s1">'x'</span><span class="p">,</span> <span class="s1">'grad of relu'</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
</code></pre></div>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/04/wp_editor_md_aeabb188e385383f5e54f4f9a2c9c38d.jpg"><img alt="" src="/assets/doc/04-ai/深度学习/激活函数/images/wp_editor_md_aeabb188e385383f5e54f4f9a2c9c38d.jpg"/></a></p>
<h3 id="sigmoid">Sigmoid 激活函数</h3>
<p>公式： $ \sigma(x) = \frac{1}{1 + e^{-x}} $</p>
<div class="codehilite"><pre><span></span><code><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="s1">'x'</span><span class="p">,</span> <span class="s1">'sigmoid(x)'</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
</code></pre></div>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/04/wp_editor_md_abf695f319812cceac4897f81fba88f7.jpg"><img alt="" src="/assets/doc/04-ai/深度学习/激活函数/images/wp_editor_md_abf695f319812cceac4897f81fba88f7.jpg"/></a></p>
<h3 id="tanh">Tanh 激活函数</h3>
<p>公式： $ \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $</p>
<div class="codehilite"><pre><span></span><code><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="s1">'x'</span><span class="p">,</span> <span class="s1">'tanh(x)'</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
</code></pre></div>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/04/wp_editor_md_0ce2fd93e7cd10da5ed5f8703b0e8d13.jpg"><img alt="" src="/assets/doc/04-ai/深度学习/激活函数/images/wp_editor_md_0ce2fd93e7cd10da5ed5f8703b0e8d13.jpg"/></a></p>
<h3 id="softmax">Softmax 激活函数</h3>
<p>公式： $ \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} $</p>
<p>在前面章节中，我们使用softmax用于线性回归的多分类，但其实softmax也可以看做一种激活函数。 softmax将神经网络的输出转换为概率分布，确保每个类别的输出值在0到1之间，且所有类别的概率和为1。如z=[2.0,1.0,0.1] 经过softmax计算转化后得[0.7,0.2,0.1]，如果神经网络的输出为三个类别的得分，表示第一个类别的预测概率最大，约为70%。</p>
<p>总结来说，Softmax 是一种激活函数，它专门用于多分类问题的输出层，帮助模型生成一个概率分布，便于做分类决策。</p></div>
  <div class="post-nav">
    <a class="prev" href="/前向传播-反向传播和计算图.html">← 前向传播、反向传播和计算图</a>
    <a class="next" href="/sotfmax回归实现.html">sotfmax回归实现 →</a>
  </div>
</article>

      </section>

      <aside class="right-panel">
        
      </aside>
    </main>

    <footer class="site-footer">
      <div class="container">Copyright ©2022-2025 laumy 版权所有</div>
    </footer>

    <script src="/assets/site.js"></script>
  </body>
  </html>

