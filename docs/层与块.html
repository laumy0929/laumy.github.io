<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>层与块 - Laumy的技术栈</title>
    <link rel="stylesheet" href="../../assets/style.css">
    <!-- MathJax支持LaTeX数学公式 -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          ignoreHtmlClass: 'tex2jax_ignore',
          processHtmlClass: 'tex2jax_process'
        }
      };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <header class="site-header">
      <div class="container">
        <a class="logo" href="../../">Laumy的技术栈</a>
        <div class="search">
          <input id="search-input" type="text" placeholder="输入关键词回车搜索">
        </div>
        <div class="theme-toggle" title="切换主题" id="theme-toggle">☾</div>
        <nav class="top-nav">
          <div class="nav-item"><a href="../../">首页</a></div>
        </nav>
      </div>
    </header>

    <main class="container layout">
      <aside class="left-nav">
        
        <div class="card">
          <div class="card-title">文章目录</div>
          <nav id="toc"><ul><li><a href="#_1">层</a><ul><li><a href="#_2">全连接层</a></li><li><a href="#_3">激活函数层</a></li><li><a href="#_4">自定义层</a></li></ul></li><li><a href="#_7">块</a><ul><li><a href="#sequential">Sequential容器</a></li><li><a href="#_8">自定义块</a></li><li><a href="#_9">复杂块</a></li></ul></li><li><a href="#_10">参数管理</a><ul><li><a href="#_11">参数访问</a></li><li><a href="#_12">参数初始化</a></li><li><a href="#_13">参数绑定</a></li><li><a href="#_14">参数存储</a></li></ul></li></ul></nav>
        </div>
        
      </aside>

      <section class="content">
        
<article class="card post">
  <h1>层与块</h1>
  <div class="meta">2025-05-03 · ai</div>
  <div class="post-content"><p>简单来说，如下图，第一个图中间5个神经元组成了一个层。第二图3个层组成了块。第三个图中3个块组成了整个模型。</p>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/05/wp_editor_md_63a09ff6fb21e747545f4d7ca1343104.jpg"><img alt="" src="assets/doc/04-ai/深度学习/层与块/images/wp_editor_md_63a09ff6fb21e747545f4d7ca1343104.jpg"/></a></p>
<h2 id="_1">层</h2>
<p>层是神经网络的基本计算单元，负责对输入数据进行特定形式的变换，如线性映射、非线性激活等。其主要的功能是接收输入数据，生成输出结果。其中包含学习参数（如全连接层的权重和偏置）或无参数操作（如激活函数），输出形状可能与输入不同，例如全连接层将维度din映射到dout。</p>
<h3 id="_2">全连接层</h3>
<div class="codehilite"><pre><span></span><code><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>  <span class="c1"># 输入维度4，输出维度5</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>    <span class="c1"># 输入形状(3,4)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>        <span class="c1"># 输出形状(3,5) :ml-citation{ref="1,3" data="citationList"}</span>
</code></pre></div>
<p>nn.Linerar(4, 5)：这里传入两个参数，第一个参数表示输入数据特征维度（示例是4），第二个参数表示输出特征维度(示例是5)。注意这里是特征维度，而不是样本个数,比如这里的特征维度是4，可以输入[2,4],[6,4]即2行4列或6行4列的数据样本。</p>
<h3 id="_3">激活函数层</h3>
<div class="codehilite"><pre><span></span><code><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>        <span class="c1"># 无参数操作</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">]))</span>  <span class="c1"># 输出[0, 2, 0] :ml-citation{ref="3,5" data="citationList"}</span>
</code></pre></div>
<p>激活函数层也是单独的一层。激活函数层是神经网络中用于引入非线性的部分，它的作用是帮助网络学习到更加复杂的函数映射。没有激活函数，神经网络只能表示线性函数，而引入非线性后，神经网络可以表示更复杂的模式，从而在各种任务（如分类、回归等）中表现得更好。</p>
<h3 id="_4">自定义层</h3>
<p>在神经网络中，自定义层是用户根据具体任务需求自定义实现的层。与内置层（全连接层、卷积层）不同，自定义层可以根据特定的逻辑或行为来扩展模型。它允许你在训练和推理过程中执行特殊的操作或改变标准层的行为。使用自定义层可以使某些模型进行特殊计算，比如自定义正则化、损失函数或特殊的激活函数等。</p>
<p>在pytorch中如何实现自定义层，通常是通过继承torch.nn.Module类来实现的，需要定义的内容如下：</p>
<ul>
<li><strong>init</strong>:定义层需要的参数或子层。</li>
<li>forward:定义数据如何通过该层传递并执行相应的计算。</li>
</ul>
<h4 id="_5">无参数层</h4>
<p>无参数层不包含任何需要训练的参数，通常用于执行某些固定的操作或计算。比如激活函数、归一化操作、数学变换等。</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1">#继承nn.Module</span>
<span class="k">class</span> <span class="nc">custom_relu</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">custom_relu</span><span class="p">()</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="n">output_data</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_data</span><span class="p">)</span>
</code></pre></div>
<p>代码运行结果如下：</p>
<div class="codehilite"><pre><span></span><code><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.9986</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8549</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2031</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.8380</span><span class="p">,</span>  <span class="mf">0.6925</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9164</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.5807</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5719</span><span class="p">,</span>  <span class="mf">1.1864</span><span class="p">]])</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.9986</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.8380</span><span class="p">,</span> <span class="mf">0.6925</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.5807</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">1.1864</span><span class="p">]])</span>
</code></pre></div>
<h4 id="_6">带参数的层</h4>
<p>参数层包含可学习的按时，通常执行一些依赖于权重或偏置的计算，比如线性变换、卷积等。参数层通常会在训练过程中优化这些参数。</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">custom_linear_layer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">output_dim</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">custom_linear_layer</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="n">output_data</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_data</span><span class="p">)</span>
</code></pre></div>
<p>运行结果如下：</p>
<div class="codehilite"><pre><span></span><code><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.7047</span><span class="p">,</span>  <span class="mf">1.8763</span><span class="p">,</span>  <span class="mf">1.8934</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1341</span><span class="p">,</span>  <span class="mf">0.4411</span><span class="p">,</span>  <span class="mf">0.2252</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.0531</span><span class="p">,</span>  <span class="mf">0.2556</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9485</span><span class="p">,</span>  <span class="mf">1.9396</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3373</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4364</span><span class="p">,</span>  <span class="mf">0.4522</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3176</span><span class="p">]])</span>

<span class="n">tensor</span><span class="p">([[</span> <span class="mf">2.2790</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5707</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0157</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3939</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.7449</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6362</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.1973</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3335</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.3929</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5201</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div>
<h2 id="_7">块</h2>
<p>块是由多个层组成的复合模块，用于封装重复或复杂功能的代码逻辑，实现模型结构的模块化。包含前向传播逻辑forward的方法、可嵌套其他块或层形成层次化的结构，继承自nn.Module，支持参数管理和自动梯度计算。</p>
<h3 id="sequential">Sequential容器</h3>
<div class="codehilite"><pre><span></span><code><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># 包含3个子层：线性→激活→线性 :ml-citation{ref="6,7" data="citationList"}</span>
</code></pre></div>
<p>Sequential容器用于按顺序定义一个神经网络模块，它将各个子模块按照定义顺序组合在一起，从而实现前向传播。</p>
<ul>
<li>输入：假设输入时一个形状为(batch_size, 4)的张量，表示batch_size个样本，每个样本有4个特征。</li>
<li>第一个线性层：输入通过第一个nn.Linear(4, 5), 输出形状变为(batch_size, 5)。</li>
<li>ReLu激活函数：输出经过第一个nn.ReLU,所有负数变为0，正数保持不变，输出仍为形状(batch_size, 5)。</li>
<li>第二个线性层：经过第二个nn.Linear(5, 3)，输出形状变为(batch_size， 3)。</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>  <span class="c1"># 输入样本是4个特征， 转换为5个特征</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="c1">#输出3个特征</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="n">output_data</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_data</span><span class="p">)</span>
</code></pre></div>
<p>运行结果：</p>
<div class="codehilite"><pre><span></span><code><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.3054</span><span class="p">,</span>  <span class="mf">1.0160</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7137</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3744</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.6882</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3049</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2769</span><span class="p">,</span>  <span class="mf">0.2835</span><span class="p">]])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.4485</span><span class="p">,</span>  <span class="mf">0.6298</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1949</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.1992</span><span class="p">,</span>  <span class="mf">0.1609</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2480</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddmmBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div>
<h3 id="_8">自定义块</h3>
<p>在pytorch中，自定义块通常是通过继承nn.Module创建的自定义或模型块。可以根据需要组合多个操作或实现一些特定功能，创建属于自己的网络模块。</p>
<p>如何创建自定义块了？</p>
<ul>
<li>基层nn.Module: 需要先继承nn.Module，这是pytorch中所有神经网络模块的基类。</li>
<li>定义<strong>init</strong>方法：在<strong>init</strong>方法中定义层，例如nn.Linear、nn.Conv2d等操作并初始化他们。</li>
<li>定义forward方法：在forward方法中定义输入数据如何通过自定义层进行处理。</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">custom_block</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">custom_block</span> <span class="o">=</span> <span class="n">custom_block</span><span class="p">()</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="n">output_data</span> <span class="o">=</span> <span class="n">custom_block</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_data</span><span class="p">)</span>
</code></pre></div>
<p>运行结果如下：</p>
<div class="codehilite"><pre><span></span><code><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.4663</span><span class="p">,</span>  <span class="mf">0.9429</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2072</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7672</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.6028</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2563</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3493</span><span class="p">,</span>  <span class="mf">1.2657</span><span class="p">]])</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.0273</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1265</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2595</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.1276</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0837</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4265</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddmmBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div>
<h3 id="_9">复杂块</h3>
<p>待补充</p>
<h2 id="_10">参数管理</h2>
<p>在深度学习中，参数管理通常指的是如何管理模块中的参数，确保它们在训练过程中得到适当的更新，或者在不同阶段(如训练、验证、测试)进行适当的操作。有效的参数管理有助于提高模型训练的效率和稳定性。</p>
<h3 id="_11">参数访问</h3>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div>
<p>运行结果如下：</p>
<div class="codehilite"><pre><span></span><code><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1428</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1919</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddmmBackward0</span><span class="o">&gt;</span><span class="p">)</span>

<span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">'weight'</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.3178</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2009</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1120</span><span class="p">,</span>  <span class="mf">0.1502</span><span class="p">,</span>  <span class="mf">0.0054</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0864</span><span class="p">,</span>  <span class="mf">0.2142</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0564</span><span class="p">]])),</span> <span class="p">(</span><span class="s1">'bias'</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.0326</span><span class="p">]))])</span>  <span class="c1">#打印.state_dirct()</span>

<span class="o">&lt;</span><span class="k">class</span> <span class="err">'</span><span class="nc">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">Parameter</span><span class="s1">'&gt; #-打印.bias</span>
<span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.0326</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.0326</span><span class="p">])</span> <span class="c1">#打印.bias.data</span>

<span class="kc">None</span>   <span class="c1">#打印.weight.grad</span>
</code></pre></div>
<p>也可以使用下面的一次性访问所有参数</p>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="p">[(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()])</span>
<span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="p">[(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()])</span>
</code></pre></div>
<p>运行结果：</p>
<div class="codehilite"><pre><span></span><code><span class="p">(</span><span class="s1">'weight'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span> <span class="p">(</span><span class="s1">'bias'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">]))</span>
<span class="p">(</span><span class="s1">'0.weight'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span> <span class="p">(</span><span class="s1">'0.bias'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">]))</span> <span class="p">(</span><span class="s1">'2.weight'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">]))</span> <span class="p">(</span><span class="s1">'2.bias'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div>
<p>另外可以使用print打印模型的结构</p>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">运行如下</span><span class="err">：</span>
<span class="n">Sequential</span><span class="p">(</span>
  <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">()</span>
  <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="_12">参数初始化</h3>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">init_normal</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

<span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_normal</span><span class="p">)</span>
<span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>
<p>运行结果如下：</p>
<div class="codehilite"><pre><span></span><code><span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.9942</span><span class="p">,</span> <span class="mf">0.9995</span><span class="p">,</span> <span class="mf">0.9971</span><span class="p">,</span> <span class="mf">0.9903</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">))</span>
</code></pre></div>
<p>上面的代码定义了一个init_normal函数，改变了weight和bias，初始化为标准差0.01的高斯随机变量且将参数设置为0。</p>
<h3 id="_13">参数绑定</h3>
<p>所谓参数绑定，就是将多个层间使用共享参数，下面看示例。</p>
<div class="codehilite"><pre><span></span><code><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">shared</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">shared</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">net</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">net</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">net</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<p>运行结果如下：</p>
<div class="codehilite"><pre><span></span><code><span class="n">Sequential</span><span class="p">(</span>
  <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">()</span>
  <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">()</span>
  <span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">()</span>
  <span class="p">(</span><span class="mi">6</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">])</span>
</code></pre></div>
<p>可以看到第2层和第4层的参数是一样的，他们不仅值相等，当改变其中一个参数，另一个参数也会一起改变为一样的值。</p>
<h3 id="_14">参数存储</h3>
<p>在pytorch中可以调用save和load保存和读取文件，示例如下。</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">'x-file'</span><span class="p">)</span>

<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'x-file'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>

<span class="n">打印如下</span><span class="err">：</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</code></pre></div>
<p>在训练过程中，可以将参数进行保存，下面是示例。</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">nlp</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">'nlp.params'</span><span class="p">)</span>

<span class="n">clone</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">()</span>
<span class="n">clone</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'nlp.params'</span><span class="p">))</span>
<span class="n">clone</span><span class="o">.</span><span class="n">eval</span>

<span class="n">Y_clone</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Y_clone</span> <span class="o">==</span> <span class="n">Y</span>

<span class="n">打印结果</span><span class="err">：</span>

<span class="n">nlp</span><span class="p">(</span>
  <span class="p">(</span><span class="n">hidden</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="n">output</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.3927</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9475</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6044</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5835</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5661</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4240</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4481</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0627</span><span class="p">,</span>
          <span class="mf">0.7437</span><span class="p">,</span>  <span class="mf">1.0465</span><span class="p">,</span>  <span class="mf">0.1806</span><span class="p">,</span>  <span class="mf">0.1096</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2199</span><span class="p">,</span>  <span class="mf">1.1642</span><span class="p">,</span>  <span class="mf">1.0633</span><span class="p">,</span>  <span class="mf">1.3925</span><span class="p">,</span>
          <span class="mf">0.3849</span><span class="p">,</span>  <span class="mf">0.9443</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4781</span><span class="p">,</span>  <span class="mf">0.6522</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.2506</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7369</span><span class="p">,</span>  <span class="mf">0.7148</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3734</span><span class="p">,</span>  <span class="mf">1.3801</span><span class="p">,</span>  <span class="mf">0.4163</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3707</span><span class="p">,</span>  <span class="mf">0.5407</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.1734</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1068</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1630</span><span class="p">,</span>  <span class="mf">1.2899</span><span class="p">,</span>  <span class="mf">0.4753</span><span class="p">,</span>  <span class="mf">0.7332</span><span class="p">,</span>  <span class="mf">0.5401</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4011</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.5356</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5833</span><span class="p">,</span>  <span class="mf">0.8288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5439</span><span class="p">]])</span>

<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.6972</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0666</span><span class="p">,</span>  <span class="mf">0.5621</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4620</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1545</span><span class="p">,</span>  <span class="mf">0.2283</span><span class="p">,</span>  <span class="mf">0.1647</span><span class="p">,</span>  <span class="mf">0.1879</span><span class="p">,</span>
          <span class="mf">0.1907</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1658</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2174</span><span class="p">,</span>  <span class="mf">0.2586</span><span class="p">,</span>  <span class="mf">0.2867</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2213</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0090</span><span class="p">,</span>  <span class="mf">0.0687</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0382</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0477</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.3194</span><span class="p">,</span>  <span class="mf">0.1438</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddmmBackward0</span><span class="o">&gt;</span><span class="p">)</span>

<span class="n">tensor</span><span class="p">([[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span>
        <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]])</span>
</code></pre></div>
<p>上面的示例中，先调用torch.save(net.state_dirc(), 'npl.params')将参数保存起来，然后接着通过load_state_dict(torch.load('npl.params'))，将参数读取出来。通过保存参数的方法，可以将训练的实例化进行备份，从上一次保存的参数接着训练。</p>
<p>本文来自： <a href="https://zh.d2l.ai/" title="&lt;动手学深度学习 V2&gt;">&lt;动手学深度学习 V2&gt;</a> 的学习笔记</p></div>
  <div class="post-nav">
    <a class="prev" href="../../非暴力沟通.html">← 非暴力沟通</a>
    <a class="next" href="../../梯度计算.html">梯度计算 →</a>
  </div>
</article>

      </section>

      <aside class="right-panel">
        
      </aside>
    </main>

    <footer class="site-footer">
      <div class="container">Copyright ©2022-2025 laumy 版权所有</div>
    </footer>

    <script src="../../assets/site.js"></script>
  </body>
  </html>

