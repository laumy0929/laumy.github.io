<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>sotfmax回归实现 - Laumy的技术栈</title>
    <link rel="stylesheet" href="../../assets/style.css">
    <!-- MathJax支持LaTeX数学公式 -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          ignoreHtmlClass: 'tex2jax_ignore',
          processHtmlClass: 'tex2jax_process'
        }
      };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <header class="site-header">
      <div class="container">
        <a class="logo" href="../../">Laumy的技术栈</a>
        <div class="search">
          <input id="search-input" type="text" placeholder="输入关键词回车搜索">
        </div>
        <div class="theme-toggle" title="切换主题" id="theme-toggle">☾</div>
        <nav class="top-nav">
          <div class="nav-item"><a href="../../">首页</a></div>
        </nav>
      </div>
    </header>

    <main class="container layout">
      <aside class="left-nav">
        
        <div class="card">
          <div class="card-title">文章目录</div>
          <nav id="toc"><ul><li><a href="#sotfmax">什么是sotfmax回归</a><ul><li><a href="#softmax">softmax函数</a></li><li><a href="#_1">交叉熵损失函数</a></li></ul></li><li><a href="#softmax_1">softmax实现示例</a><ul><li><a href="#_2">数据读取</a></li><li><a href="#_3">定义模型</a></li><li><a href="#_5">定义损失函数</a></li><li><a href="#_6">分类精度</a></li><li><a href="#_7">训练</a></li><li><a href="#_8">预测</a></li></ul></li><li><a href="#_9">总结</a><ul></ul></li></ul></nav>
        </div>
        
      </aside>

      <section class="content">
        
<article class="card post">
  <h1>sotfmax回归实现</h1>
  <div class="meta">2025-04-27 · ai</div>
  <div class="post-content"><h2 id="sotfmax">什么是sotfmax回归</h2>
<p>Softmax回归（Softmax Regression），也叫多项逻辑回归，是一种用于多分类问题的分类算法。它是对逻辑回归（Logistic Regression）的一种扩展，适用于处理输出类别数大于2的情况。Softmax回归通过使用Softmax函数来将每个类别的输出转化为一个概率分布，使得输出值能够表示每个类别的概率，并且所有类别的概率之和为1。</p>
<p>举个例子：假设有一个包含3个类别的多分类问题：苹果、香蕉、橙子。对于每个输入样本（例如一张图片），Softmax回归模型会输出三个值（每个类别的概率），也就是概率分布。例如：</p>
<ul>
<li>苹果的概率：0.6</li>
<li>香蕉的概率：0.3</li>
<li>橙子的概率：0.1</li>
</ul>
<p>这些概率加起来等于1，模型会将输入样本分类为苹果（因为概率最大）。</p>
<h3 id="softmax">softmax函数</h3>
<p>对于每个类别$ k $ ，我们会计算一个得分$ z_k $，然后将这个得分转化为概率。得分通常是由输入数据$ \mathbf{x} $与对应类别的权重向量$ \mathbf{w}_k $ 的线性组合给出的：$ z_k = \mathbf{w}_k^T \mathbf{x} + b_k $, 其中，$ \mathbf{w}_k $ 是第$ k$ 个类别的权重，$ b_k$ 是偏置项，$ \mathbf{x} $ 是输入特征向量。Softmax函数用于将这些得分$ z_k $转换成概率。</p>
<p>Softmax函数的形式如下：$ P(y = k | \mathbf{x}) = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}} $ 。</p>
<ul>
<li>$ P(y = k | \mathbf{x}) ) 是输入 ( \mathbf{x} $ 属于类别k的概率。</li>
<li>$ z_k $ 是类别 $ k $ 的得分。</li>
<li>$ \sum_{j=1}^K e^{z_j} $ 是所有类别得分的指数函数的和，确保概率和为1。</li>
</ul>
<h3 id="_1">交叉熵损失函数</h3>
<p>为了训练Softmax回归模型，我们使用交叉熵损失函数来评估模型预测与真实标签之间的差异。交叉熵损失函数的公式如下：$ L(\theta) = - \sum_{i=1}^N \sum_{k=1}^K y_{ik} \log P(y_k = 1 | \mathbf{x}_i) $</p>
<p>其中： - $ N $ 是训练集中的样本数。 - $ y_{ik} $ 是样本 $ i $是否属于类别 $ k $ 的标签（通常是1或0）。 - $ P(y_k = 1 | \mathbf{x}_i) $ 是输入 $ \mathbf{x}_i $ 属于类别 $ k $ 的概率。</p>
<h2 id="softmax_1">softmax实现示例</h2>
<h3 id="_2">数据读取</h3>
<div class="codehilite"><pre><span></span><code><span class="n">pip</span> <span class="n">install</span> <span class="n">d2l</span><span class="o">==</span><span class="mf">0.16</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">load_data_fashion_mnist</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
</code></pre></div>
<p>这里直接使用了d2l.load_data_fashion_mnist() 函数加载 Fashion-MNIST 数据集。load_data_fashion_mnist 是 d2l 库中的一个工具函数，用于加载 Fashion-MNIST 数据集并返回训练集和测试集的数据迭代器。train_iter 是训练集的迭代器。test_iter 是测试集的迭代器。数据迭代器是用于在模型训练和评估过程中批量加载数据的对象。batch_size 参数指定了每个批次包含多少个样本。</p>
<p>可以用下面的示例代码打印输入的数据</p>
<div class="codehilite"><pre><span></span><code><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_iter</span><span class="p">:</span>
    <span class="k">break</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">'Label: </span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">label</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">n</span><span class="p">]]</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">show_images</span><span class="p">(</span><span class="n">X_selected</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">titles</span><span class="o">=</span><span class="n">titles</span><span class="p">)</span>
</code></pre></div>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/04/wp_editor_md_c498a5a2f4ecdcf9c2c6979444cfe20b.jpg"><img alt="" src="assets/doc/04-ai/深度学习/sotfmax回归实现/images/wp_editor_md_c498a5a2f4ecdcf9c2c6979444cfe20b.jpg"/></a></p>
<h3 id="_3">定义模型</h3>
<h4 id="sotfmax_1">sotfmax函数</h4>
<p>计算softmax的步骤如下：</p>
<ul>
<li>对每个项求幂（使用exp）</li>
<li>对每一行求和（小批量中每个样本是一行），得到每个样本的规范化常数</li>
<li>将每一行除以其规范化常数，确保结果的和为1</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">X_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">X_exp</span><span class="p">)</span>
    <span class="n">partition</span> <span class="o">=</span> <span class="n">X_exp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">partition</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X_exp</span> <span class="o">/</span> <span class="n">partition</span>
</code></pre></div>
<p>示例如下：</p>
<div class="codehilite"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="c1">#使用正态分布生成2行5列的矩阵</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_prob</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_prob</span><span class="p">,</span> <span class="n">X_prob</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#生成的数据</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.3141</span><span class="p">,</span>  <span class="mf">0.5186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6949</span><span class="p">,</span>  <span class="mf">0.5918</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2370</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.3814</span><span class="p">,</span>  <span class="mf">0.8092</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1959</span><span class="p">,</span>  <span class="mf">0.7489</span><span class="p">,</span>  <span class="mf">1.8790</span><span class="p">]])</span>

<span class="c1">#torch.exp(X)：对矩阵中每个数据求e^x指数运算后的结果</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">1.3690</span><span class="p">,</span> <span class="mf">1.6797</span><span class="p">,</span> <span class="mf">0.4991</span><span class="p">,</span> <span class="mf">1.8072</span><span class="p">,</span> <span class="mf">0.1068</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.6829</span><span class="p">,</span> <span class="mf">2.2460</span><span class="p">,</span> <span class="mf">0.8221</span><span class="p">,</span> <span class="mf">2.1146</span><span class="p">,</span> <span class="mf">6.5472</span><span class="p">]])</span>

<span class="c1">#X_exp.sum(1, keepdim=True)： 对每一行求和</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">5.4618</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">12.4129</span><span class="p">]])</span>

<span class="c1">#将每一行除以其规范化常数，确保结果的和为1</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.2506</span><span class="p">,</span> <span class="mf">0.3075</span><span class="p">,</span> <span class="mf">0.0914</span><span class="p">,</span> <span class="mf">0.3309</span><span class="p">,</span> <span class="mf">0.0196</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">0.0550</span><span class="p">,</span> <span class="mf">0.1809</span><span class="p">,</span> <span class="mf">0.0662</span><span class="p">,</span> <span class="mf">0.1704</span><span class="p">,</span> <span class="mf">0.5275</span><span class="p">]]),</span>

<span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]))</span>
</code></pre></div>
<h4 id="_4">模型和参数</h4>
<div class="codehilite"><pre><span></span><code><span class="n">num_inputs</span> <span class="o">=</span> <span class="mi">784</span>
<span class="n">num_outputs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">net</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div>
<p>模型还是使用的是线性模式，只是在线性模型的基础上再加了一个softmax函数。模型的数学表示为：$ \hat{y} = \text{softmax}(X W + b) $</p>
<ul>
<li>$ X \in \mathbb{R}^{n \times 784} $ 是输入样本矩阵，$ n $ 是样本数量。</li>
<li>$ W \in \mathbb{R}^{784 \times 10} $ 是权重矩阵。</li>
<li>$ b \in \mathbb{R}^{10} $ 是偏置向量。</li>
<li>$ \hat{y} \in \mathbb{R}^{n \times 10} $ 是输出矩阵，其中每一行是一个样本的预测类别概率。</li>
</ul>
<p><code>softmax</code> 函数的公式为：$ \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}} $</p>
<p>其中 $ z_i $ 是某一类别的得分，$ j $ 遍历所有类别（在这个例子中是 10 个类别）。通过 <code>softmax</code> 函数，每个输出都会被转换为一个概率，所有类别的概率加起来为 1。</p>
<h3 id="_5">定义损失函数</h3>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)),</span> <span class="n">y</span><span class="p">])</span>
</code></pre></div>
<ul>
<li>y_hat：这是模型的预测输出，通常是一个经过 softmax 函数处理的概率分布。y_hat 的形状通常是 (batch_size, num_classes)，其中 batch_size 是样本数量，num_classes 是类别数量。每一行表示一个样本对各个类别的预测概率。</li>
<li>y：这是实际标签的索引，形状为 (batch_size,)，表示每个样本的真实类别的索引。</li>
<li>y_hat[range(len(y_hat)), y]：这是通过 y 中的类别索引提取 y_hat 中对应类别的预测概率。range(len(y_hat)) 生成一个从 0 到 batch_size-1 的索引序列，表示每个样本。通过 y 索引，获取每个样本对应类别的概率值。</li>
<li>torch.log(...)：对提取的预测概率取对数。交叉熵损失函数中有一个 log 操作，它衡量了预测概率和真实标签之间的差异。</li>
<li>负号：交叉熵是通过负对数似然（negative log-likelihood）计算的，因此需要对结果取负。</li>
</ul>
<p>损失函数的公式为：$ L = - \frac{1}{n} \sum_{i=1}^{n} \log(\hat{y}_{i, y_i}) $, 通过对每个样本的预测概率取对数，并对所有样本的对数损失求和再取负值。</p>
<h3 id="_6">分类精度</h3>
<p>分类精度= 样本预测正确数量除以样本总数（len(y)）。 也可以理解是预测对的概率，比如输入样本图片识别正确数为1，总样本数2时，精度为 1/2 = 0.5。</p>
<p>先看看例子，y_hat模型的预测输出，通常是一个二维矩阵，形状为 (样本数, 类别数)。例如，2个样本（输入的图片）3个类别(猫、狗、猪)的输出可能是 [[0.1, 0.2, 0.7], [0.3, 0.4, 0.3]]，即每个样本对应输出的一个概率分布，样本1对应的概率分布[0.1, 0.2, 0.7]，样本2对应的概率分布是[0.3, 0.4, 0.3]，而真实的标签y是一个一维向量，每个元素表示对应样本的正确类别索引，如[2, 1]，其中2代表的是狗，1代表猫。</p>
<p>那y_hat和y怎么做比较和转换了？ 解决的办法就是，我们取每个样本概率分布中最大概率的索引，也就是通过 argmax(axis=1) 沿着行方向（即每个样本）找到概率最大的类别索引。例如，[[0.1, 0.2, 0.7], [0.3, 0.4, 0.3]] 会得到 [2, 1]， 即第一行最大是0.7，索引位置是2，第二行最大是0.4，级索引是1。有了这样的结果，就可以y_hat和y做比较了，比如y=[2,1], 那么y_hat输出结果是[2,1]，那么表示全部预测对。</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_hat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1">#y_hat将输出索引如[2,1],下面结算的是y_hat和y进行比较，返回正确的个数。</span>
    <span class="n">cmp</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">cmp</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</code></pre></div>
<p>因此上面这个函数，最终返回的是正确的个数，比如y_hat = [[0.1, 0.2, 0.7], [0.3, 0.4, 0.3]],y是[2,2]经过accuracy函数处理后，返回的是结果是1， 因为y_hat = y_hat.argmax(axis=1)计算后，返回的是[2,1]，与实际的标签[2,2]有一个不对，即第二个样本预测错了。那么最终的分类精度就等于1/2 = 0.5。</p>
<h3 id="_7">训练</h3>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">train_epoch_ch3</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">updater</span><span class="p">):</span>
    <span class="c1"># 将模型设置为训练模式</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="c1"># 训练损失总和、训练准确度总和、样本数</span>
    <span class="n">metric</span> <span class="o">=</span> <span class="n">Accumulator</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_iter</span><span class="p">:</span>
        <span class="c1"># 计算梯度并更新参数</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">updater</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
            <span class="c1"># 使用PyTorch内置的优化器和损失函数</span>
            <span class="n">updater</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">l</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">updater</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># 使用定制的优化器和损失函数</span>
            <span class="n">l</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">updater</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">metric</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">sum</span><span class="p">()),</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
    <span class="c1"># 返回训练损失和训练精度</span>
    <span class="k">return</span> <span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">train_ch3</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">updater</span><span class="p">):</span>
    <span class="n">animator</span> <span class="o">=</span> <span class="n">Animator</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                        <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="s1">'train loss'</span><span class="p">,</span> <span class="s1">'train acc'</span><span class="p">,</span> <span class="s1">'test acc'</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">train_metrics</span> <span class="o">=</span> <span class="n">train_epoch_ch3</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">updater</span><span class="p">)</span>
        <span class="o">//</span><span class="n">返回训练损失和训练精度</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">evaluate_accuracy</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
        <span class="o">//</span><span class="n">返回的是测试精度</span>
        <span class="n">animator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">train_metrics</span> <span class="o">+</span> <span class="p">(</span><span class="n">test_acc</span><span class="p">,))</span>
        <span class="o">//</span><span class="n">将其绘制到图像上</span><span class="err">。</span>
    <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">train_metrics</span>
    <span class="k">assert</span> <span class="n">train_loss</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">train_loss</span>
    <span class="k">assert</span> <span class="n">train_acc</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">train_acc</span> <span class="o">&gt;</span> <span class="mf">0.7</span><span class="p">,</span> <span class="n">train_acc</span>
    <span class="k">assert</span> <span class="n">test_acc</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">test_acc</span> <span class="o">&gt;</span> <span class="mf">0.7</span><span class="p">,</span> <span class="n">test_acc</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="k">def</span> <span class="nf">updater</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">d2l</span><span class="o">.</span><span class="n">sgd</span><span class="p">([</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">train_ch3</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">cross_entropy</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">updater</span><span class="p">)</span>
</code></pre></div>
<p>下面是训练的过程显示：</p>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/04/wp_editor_md_3a3c450df8d8bfcc93b0654e2265e52c.jpg"><img alt="" src="assets/doc/04-ai/深度学习/sotfmax回归实现/images/wp_editor_md_3a3c450df8d8bfcc93b0654e2265e52c.jpg"/></a></p>
<ul>
<li>train loss: 训练损失，也就是损失函数的结果。是模型在训练集上的平均损失值，通常使用损失函数来衡量。例如，常用的交叉熵损失（cross-entropy loss）或均方误差（mean squared error）。损失越小，说明模型在训练数据上的表现越好。它反映了模型预测值与真实标签之间的差距。</li>
<li>train acc: Training Accuracy, 训练精度。是模型在训练集上的正确预测的比例。它通过比较模型的预测结果和真实标签来计算。训练精度=正确预测的样本数量/总样本数量。训练准确度越高，说明模型在训练数据上的拟合程度越好。训练准确度反映了模型对训练集的学习能力。</li>
<li>test acc: Test Accuracy,测试精度。是指模型在未见过的测试集上的准确度。它与训练准确度不同，测试集用来评估模型的泛化能力。测试准确度反映了模型对新数据的预测能力。如果测试准确度高，说明模型不仅在训练集上表现好，而且具有较强的泛化能力，能够适应未见过的数据。</li>
</ul>
<h3 id="_8">预测</h3>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict_ch3</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">test_iter</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">trues</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">get_fashion_mnist_labels</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">get_fashion_mnist_labels</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="n">true</span> <span class="o">+</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span> <span class="o">+</span> <span class="n">pred</span> <span class="k">for</span> <span class="n">true</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">trues</span><span class="p">,</span> <span class="n">preds</span><span class="p">)]</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">show_images</span><span class="p">(</span>
        <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">titles</span><span class="o">=</span><span class="n">titles</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">n</span><span class="p">])</span>

<span class="n">predict_ch3</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
</code></pre></div>
<p>使用训练好的模型，来预测实际的效果：</p>
<p><a href="https://www.laumy.tech/wp-content/uploads/2025/04/wp_editor_md_613f320a2cdf3039cec189d756c20eb9.jpg"><img alt="" src="assets/doc/04-ai/深度学习/sotfmax回归实现/images/wp_editor_md_613f320a2cdf3039cec189d756c20eb9.jpg"/></a></p>
<h2 id="_9">总结</h2>
<div class="codehilite"><pre><span></span><code><span class="n">一</span><span class="err">、</span><span class="n">公式和代码</span>

<span class="n">公式</span><span class="err">：</span><span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">WX</span><span class="o">+</span><span class="n">b</span><span class="p">)</span>
<span class="n">代码实现</span><span class="err">：</span><span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="n">二</span><span class="err">、</span><span class="n">输入和输出示例</span>

<span class="n">输入</span><span class="err">：</span>
<span class="n">X</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span><span class="o">--&gt;</span><span class="n">X</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">256</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span>
<span class="n">由于WX要满足矩阵乘</span><span class="err">，</span><span class="n">所以要把X做处理X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="n">W</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>

<span class="n">输出</span><span class="p">:</span>

<span class="n">y_hat</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>   <span class="o">--&gt;</span><span class="p">([</span><span class="mi">256</span><span class="p">,</span><span class="mi">784</span><span class="p">])</span><span class="o">*</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span> <span class="o">=</span> <span class="p">([</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span><span class="n">矩阵相乘</span>

<span class="n">下面是打印第一行的结果</span><span class="err">，</span><span class="n">也就是对应输入第一个样本的预测结果</span><span class="err">。</span> <span class="n">最后为0</span><span class="mf">.99613</span><span class="err">，</span><span class="n">如果最后一项是代表是shirt</span><span class="err">，</span><span class="n">但是表示第一个样本就是shirt</span><span class="err">。</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">4.8291e-06</span><span class="p">,</span> <span class="mf">1.2489e-07</span><span class="p">,</span> <span class="mf">3.7127e-06</span><span class="p">,</span> <span class="mf">2.1127e-07</span><span class="p">,</span> <span class="mf">1.3334e-06</span><span class="p">,</span> <span class="mf">2.6440e-03</span><span class="p">,</span>
        <span class="mf">1.9091e-05</span><span class="p">,</span> <span class="mf">8.7211e-04</span><span class="p">,</span> <span class="mf">3.2460e-04</span><span class="p">,</span> <span class="mf">9.9613e-01</span><span class="p">])</span>
</code></pre></div>
<p>本文来自： <a href="https://zh.d2l.ai/" title="&lt;动手学深度学习 V2&gt;">&lt;动手学深度学习 V2&gt;</a> 的学习笔记</p></div>
  <div class="post-nav">
    <a class="prev" href="../../激活函数.html">← 激活函数</a>
    <a class="next" href="../../线性回归实现.html">线性回归实现 →</a>
  </div>
</article>

      </section>

      <aside class="right-panel">
        
      </aside>
    </main>

    <footer class="site-footer">
      <div class="container">Copyright ©2022-2025 laumy 版权所有</div>
    </footer>

    <script src="../../assets/site.js"></script>
  </body>
  </html>

