<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>前向传播、反向传播和计算图 - Laumy的技术栈</title>
    <link rel="stylesheet" href="/assets/style.css">
    <!-- MathJax支持LaTeX数学公式 -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          ignoreHtmlClass: 'tex2jax_ignore',
          processHtmlClass: 'tex2jax_process'
        }
      };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <header class="site-header">
      <div class="container">
        <a class="logo" href="/">Laumy的技术栈</a>
        <div class="search">
          <input id="search-input" type="text" placeholder="输入关键词回车搜索">
        </div>
        <div class="theme-toggle" title="切换主题" id="theme-toggle">☾</div>
        <nav class="top-nav">
          <div class="nav-item"><a href="/">首页</a></div>
        </nav>
      </div>
    </header>

    <main class="container layout">
      <aside class="left-nav">
        
        <div class="card">
          <div class="card-title">文章目录</div>
          <nav id="toc"><ul><li><a href="#forward-propagation">前向传播（Forward Propagation）</a><ul><li><a href="#_1">模型定义</a></li><li><a href="#_2">损失函数（均方误差）</a></li><li><a href="#_3">示例</a></li><li><a href="#_4">前向计算</a></li></ul></li><li><a href="#computational-graph">计算图（Computational Graph）</a><ul></ul></li><li><a href="#backward-propagation">反向传播（Backward Propagation）</a><ul><li><a href="#_5">损失对预测值的梯度</a></li><li><a href="#_6">预测值对参数的梯度</a></li><li><a href="#_7">反向传播示例</a></li></ul></li><li><a href="#pytorch">PyTorch示例</a><ul></ul></li></ul></nav>
        </div>
        
      </aside>

      <section class="content">
        
<article class="card post">
  <h1>前向传播、反向传播和计算图</h1>
  <div class="meta">2025-05-01 · ai</div>
  <div class="post-content"><h2 id="forward-propagation">前向传播（Forward Propagation）</h2>
<p>前向传播是神经网络中从输入数据到输出预测值的计算过程。它通过逐层应用权重（W）和偏置（b），最终生成预测值 $y' $，并计算损失函数$L $。</p>
<h3 id="_1">模型定义</h3>
<p>$$ y' = W \cdot x + b $$</p>
<h3 id="_2">损失函数（均方误差）</h3>
<p>$$ L = \frac{1}{n} \sum_{i=1}^{n} (y'(i) - y_{\text{true}}(i))^2 $$</p>
<h3 id="_3">示例</h3>
<p>输入数据：$x = [1.0, 2.0] $</p>
<p>真实标签：$y_{\text{true}} = [3.0, 5.0]$</p>
<p>参数初始值：$W = 1.0, \, b = 0.5$</p>
<h3 id="_4">前向计算</h3>
<p>预测值：$y'(1) = 1.0 \cdot 1.0 + 0.5 = 1.5, \quad y'(2) = 1.0 \cdot 2.0 + 0.5 = 2.5$</p>
<p>损失函数：$L = \frac{1}{2} \left[ (1.5 - 3)^2 + (2.5 - 5)^2 \right] = \frac{1}{2} (2.25 + 6.25) = 4.25$</p>
<h2 id="computational-graph">计算图（Computational Graph）</h2>
<p>计算图是一种数据结构，用于表示前向传播中的计算过程。图中的节点代表数学操作（如加法、乘法），边代表数据流动（张量）。</p>
<p>对上述线性回归模型，计算图如下：</p>
<div class="codehilite"><pre><span></span><code><span class="n">输入</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="p">(</span><span class="n">Multiply</span><span class="w"> </span><span class="n">W</span><span class="p">)</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="p">(</span><span class="n">Add</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">预测值</span><span class="w"> </span><span class="n">t_p</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="p">(</span><span class="n">Subtract</span><span class="w"> </span><span class="n">y_true</span><span class="p">)</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">误差平方</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">求和平均</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">损失</span><span class="w"> </span><span class="n">L</span>
</code></pre></div>
<ul>
<li><strong>节点</strong>：乘法、加法、平方、求和、平均等操作。</li>
<li><strong>边</strong>：数据流（如 $ x, W, b, y', L$）。</li>
</ul>
<h2 id="backward-propagation">反向传播（Backward Propagation）</h2>
<p>反向传播是通过链式法则（Chain Rule），从损失函数$ L $开始，反向计算每个参数$（W, b）$的梯度 $( \frac{\partial L}{\partial W} ) $和$ ( \frac{\partial L}{\partial b} ) $的过程。下面以线性回归模型公式示例：</p>
<h3 id="_5">损失对预测值的梯度</h3>
<p>$$ \frac{\partial L}{\partial y'(i)} = \frac{2}{n} (y'(i) - y_{\text{true}}(i)) $$</p>
<h3 id="_6">预测值对参数的梯度</h3>
<ul>
<li>对权重 $W $：$ \frac{\partial y'(i)}{\partial W} = x(i) $</li>
<li>对偏置 $b $：$\frac{\partial y'(i)}{\partial b} = 1 $</li>
</ul>
<p><strong>合并梯度</strong></p>
<ul>
<li>
<p>权重梯度：$ \frac{\partial L}{\partial W} = \sum_{i=1}^{n} \frac{\partial L}{\partial y'(i)} \cdot \frac{\partial y'(i)}{\partial W} = \frac{2}{n} \sum_{i=1}^{n} (y'(i) - y_{\text{true}}(i)) \cdot x(i) $</p>
</li>
<li>
<p>偏置梯度：$ \frac{\partial L}{\partial b} = \sum_{i=1}^{n} \frac{\partial L}{\partial y'(i)} \cdot \frac{\partial y'(i)}{\partial b} = \frac{2}{n} \sum_{i=1}^{n} (y'(i) - y_{\text{true}}(i)) $</p>
</li>
</ul>
<h3 id="_7">反向传播示例</h3>
<p>使用前向传播的结果：</p>
<p><strong>计算误差项</strong></p>
<p>$$ y'(1) - y_{\text{true}}(1) = 1.5 - 3 = -1.5, \quad y'(2) - y_{\text{true}}(2) = 2.5 - 5 = -2.5 $$</p>
<p><strong>计算梯度</strong></p>
<ul>
<li>
<p>权重梯度：$\frac{\partial L}{\partial W} = \frac{2}{2} [(-1.5) \cdot 1.0 + (-2.5) \cdot 2.0] = 1.0 \cdot (-1.5 - 5) = -6.5 $</p>
</li>
<li>
<p>偏置梯度：$\frac{\partial L}{\partial b} = \frac{2}{2} [(-1.5) + (-2.5)] = 1.0 \cdot (-4) = -4$</p>
</li>
</ul>
<h2 id="pytorch">PyTorch示例</h2>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># 定义参数（启用梯度追踪）</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 输入数据</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">])</span>

<span class="c1"># 前向传播</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">W</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># 反向传播</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># 输出梯度</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"dL/dW: </span><span class="si">{</span><span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>  <span class="c1"># 输出 tensor(-6.5)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"dL/db: </span><span class="si">{</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>  <span class="c1"># 输出 tensor(-4.0)</span>
</code></pre></div>
<p>关键点说明</p>
<ul>
<li>动态计算图：PyTorch 在前向传播时自动构建计算图。</li>
<li>反向传播触发：调用 .backward() 后，从损失节点反向遍历图，计算所有 requires_grad=True 的张量的梯度。</li>
<li>梯度存储：梯度结果存储在张量的 .grad 属性中。</li>
</ul>
<p>总结：</p>
<table>
<thead>
<tr>
<th>概念</th>
<th>作用</th>
<th>示例中的体现</th>
</tr>
</thead>
<tbody>
<tr>
<td>前向传播</td>
<td>计算预测值和损失函数</td>
<td>$ y' = W \cdot x + b, L = 4.25 $</td>
</tr>
<tr>
<td>计算图</td>
<td>记录所有计算操作，为反向传播提供路径</td>
<td>乘法、加法、平方、求和、平均等操作组成的数据结构</td>
</tr>
<tr>
<td>反向传播</td>
<td>通过链式法则计算参数梯度</td>
<td>$ \frac{\partial L}{\partial W} = -6.5 $</td>
</tr>
</tbody>
</table>
<div class="codehilite"><pre><span></span><code><span class="n">输入</span><span class="w"> </span><span class="n">x</span><span class="w">  </span>
<span class="w">  </span><span class="err">│</span><span class="w">  </span>
<span class="w">  </span><span class="err">▼</span><span class="w">  </span>
<span class="p">[</span><span class="n">W</span><span class="o">*</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">乘法操作</span><span class="err">（</span><span class="n">计算图节点</span><span class="err">）</span><span class="w">  </span>
<span class="w">  </span><span class="err">│</span><span class="w">  </span>
<span class="w">  </span><span class="err">▼</span><span class="w">  </span>
<span class="p">[</span><span class="o">+</span><span class="n">b</span><span class="p">]</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">加法操作</span><span class="err">（</span><span class="n">计算图节点</span><span class="err">）</span><span class="w">  </span>
<span class="w">  </span><span class="err">│</span><span class="w">  </span>
<span class="w">  </span><span class="err">▼</span><span class="w">  </span>
<span class="n">预测值</span><span class="w"> </span><span class="n">y</span><span class="err">'</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="p">[</span><span class="n">平方损失</span><span class="p">]</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="n">平均损失</span><span class="w"> </span><span class="n">L</span><span class="w">  </span>
<span class="w">  </span><span class="err">│</span><span class="w">                          </span><span class="err">▲</span><span class="w">  </span>
<span class="w">  </span><span class="err">└──────────────────────────┘</span><span class="w">  </span>
<span class="w">          </span><span class="n">反向传播</span><span class="err">（</span><span class="n">梯度回传</span><span class="err">）</span>
</code></pre></div></div>
  <div class="post-nav">
    <a class="prev" href="/梯度计算.html">← 梯度计算</a>
    <a class="next" href="/激活函数.html">激活函数 →</a>
  </div>
</article>

      </section>

      <aside class="right-panel">
        
      </aside>
    </main>

    <footer class="site-footer">
      <div class="container">Copyright ©2022-2025 laumy 版权所有</div>
    </footer>

    <script src="/assets/site.js"></script>
  </body>
  </html>

